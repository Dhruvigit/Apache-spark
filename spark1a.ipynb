{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns-6xBaFrTEs",
        "outputId": "a0f77846-4085-40e1-9b30-ede50256c405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf,SparkContext"
      ],
      "metadata": {
        "id": "66NX3pmSrZ7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initalize spark context\n",
        "conf=SparkConf().setMaster(\"local\").setAppName(\"RDDExample\")"
      ],
      "metadata": {
        "id": "WyPULgoIruoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T15grbqSsX-R",
        "outputId": "823ee09a-06fc-41d4-a061-6cebcdbf891e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.conf.SparkConf at 0x7b6890f9f0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc=SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "RfDGo0Cgr9o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "mE21jwsvs9da",
        "outputId": "f4b58ca2-7aa1-4b73-c222-f9f938438458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local appName=RDDExample>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://89d49904117c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>RDDExample</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create an rdd from a list\n",
        "data=[1,2,3,4,5]\n",
        "rdd=sc.parallelize(data)"
      ],
      "metadata": {
        "id": "cDyY0OLOsBZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform a map transformation and collect action\n",
        "squared_rdd=rdd.map(lambda x:x*x)\n",
        "result=squared_rdd.collect()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3kY8kd6sGKo",
        "outputId": "e0bd9f4f-1937-4b92-9623-533b74336a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 9, 16, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#transformation and actions of rdd\n",
        "#filter even number and count them\n",
        "even_rdd=rdd.filter(lambda x:x%2==0)\n",
        "count=even_rdd.count()\n",
        "print(count)\n",
        "collect=even_rdd.collect()\n",
        "print(collect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWBKMms9sOZ4",
        "outputId": "3ba9ecd7-b6b2-40fa-ac33-db1951be7e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "[2, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PRACTICAL 1: WORD COUNT BY RDD"
      ],
      "metadata": {
        "id": "XkEFRnoRt1dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf,SparkContext"
      ],
      "metadata": {
        "id": "Pc-LAKDy8nGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf=SparkConf().setAppName(\"WordCount\").setMaster(\"local\")"
      ],
      "metadata": {
        "id": "bQrDQP0C9EUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I54WVmk9YWy",
        "outputId": "b15bca0f-7c4b-44e0-c3eb-dffe4d583bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.conf.SparkConf at 0x7b685855fa90>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc1=SparkContext(conf=conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "7OCf3lQo9Zsa",
        "outputId": "f2757bfb-ac2e-4d44-87ba-50112dd29a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=RDDExample, master=local) created by __init__ at /tmp/ipython-input-5-222670757.py:1 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-25-1478567914.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=RDDExample, master=local) created by __init__ at /tmp/ipython-input-5-222670757.py:1 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[\"Hello Dhruvi\",\"This is a toy\",\"she studies in jai hind college\",\"Agentic AI refers to AI systems capable of autonomous decision-making and action, operating with limited human oversight to achieve specific goals.\",\" Unlike traditional AI, which primarily reacts to input or follows predefined rules, agentic AI demonstrates initiative and adaptability to dynamic environments.\",\" These systems can combine various AI techniques like large language models (LLMs) and complex reasoning to handle multi-step tasks, learn from their environment, and improve their performance over time.\"]"
      ],
      "metadata": {
        "id": "dLYCyzPm9c0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize(data)"
      ],
      "metadata": {
        "id": "sdhIRVfm-l9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_rdd=rdd.flatMap(lambda sentence:sentence.split(\" \"))"
      ],
      "metadata": {
        "id": "fMDTOnv5-sbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_pair_rdd=word_rdd.map(lambda word:(word,1))"
      ],
      "metadata": {
        "id": "qUa95I9T-seT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_count_rdd=word_pair_rdd.reduceByKey(lambda x,y:x+y)"
      ],
      "metadata": {
        "id": "IrB6iWrJ-sgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts=word_count_rdd.collect()"
      ],
      "metadata": {
        "id": "vvMdks9H_KYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word,count in word_counts:\n",
        "    print(f\"{word}:{count}\")"
      ],
      "metadata": {
        "id": "o5K8sewM_Kal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "ZLE42QZs_d7k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
